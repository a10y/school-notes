Today:
distance-preserving lossy compression
(dimensionality reduction)

Last time
- (dis)similarity
- nearest neighbor problem
- k-d trees
- "curse of dimensionality"

================================================================================

Issue: natural representation of data is often high dimensional
Goal: compress to few dimensions with little loss
Solution: 1) represent data in high dims
          2) map to low dimensions
          3) solve NN in low dimensions
Not just NN; can apply any computation that involve pairwise distances

Role Model:
- choose hash function h
- map each object to h(x) mod 2

Properties
x = y => still equal
x != y => still distinct, 50%

Repeating log_2 (1 / \delta) (like rows in count-min sketch)

x = y => still equal
x != y => distinct with probability 1 - \delta

================================================================================

Euclidean distance \sqrt{\sum_j (x_j - y_j)^2}
  Random Projection
    - choose a "random vector" r_1, ..., r_k
    - map each point x \in R^k to <x, r> = \sum_{j = 1}^k x_j r_j = f_r(x)
    - analogous to the 1 bit solution for similarity

================================================================================

Recall: if X_1 ~ N(\mu_1, \sigma_1^2), and X_2 ~ N(\mu_2, \sigma_2^2)
=> X1 + X_2 ~ N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)

choose r_1, ..., r_k \in R^k ~ iid N(0, 1)

================================================================================

f_r(x) - f_r(y), x, y \in R^k
== \sum_{j = 1}^k x_j r_j - \sum_{j = 1}^k y_j r_j = \sum_{j = 1}^k r_j(x_j - y_j)

~ N(0, \sum_{j = 1}^k (x_j - y_j)^2)

Recall:
if E[x] = 0, then var(x) = E[x^2]

So. E_r[(f_r(x) - f_r(y))^2] = ||x - y||_2^2

================================================================================

Can average independent copies to reduce variance - do this d times, where for each trial you choose a random vector r

for theoretical results:

d = \theta (lg n / \varepsilon^2), where n is the # points to get (1 + \varepsilon) - approx.

================================================================================

JL Dimensionality Reduction ('84)

Input: n points in R^k
Algorithm: Choose a matrix A
              k
    <------------------->
  ^ +-+-+-+-+-+-+-+-+-+-+ +---+    +---+        
  | |                   | |   |    |   |         
d | |         A         | |   | =  |   | <-- a_i^T x
  | |                   | |   |    |   |         
  v +-+-+-+-+-+-+-+-+-+-+ | X |    +---+         
                          |   |  
                          |   |  
                          |   |  
                          |   |  
                          |   |  
                          +---+

Map each x |----> 1 / \sqrt(d) A x

Claim: euclidean dists between f_A(x)s ~ between the x's.
Fix points x, y \in R^k

||f_a(x) - f_A(y) ||_2^2 =
|| 1/\sqrt(d) A x - 1/\sqrt(d) A y ||_2^2 =
1 / d || A(x - y) ||_2^2 =
|
+-> row i = output of the ith random trial

================================================================================

Jaccard similarity
A, B \subseteq u
J(A, B) = |A \intersect B| / |A \cup B|

Minhash
- choose random permutation \pi of universe U
- map each set S \subseteq U to argmin \pi(x)
                                x \in S

Key point:
- all relative orderings of A \cup B equally likely
- minhash A = minhash B
- prob of this = |A \intersect B| / |A \cup B| = J(A, B).


